{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You'll learn\n",
    "\n",
    "## ML methods\n",
    "\n",
    "- Deep NN with keras\n",
    "\n",
    "Course [slides](https://github.com/Course-bigDataAndML/MLCourse-INFN-2022/blob/master/Slides/Day3/Big%20data%20science%20-%20Day%203%20-%20INFN%202022.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "# Check out these custom functions\n",
    "from custom_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember to start spark by clicking on the square icon all the way to the right\n",
    "\n",
    "Setup and restart the Spark context with our configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# get token to authenticate to minio storage\n",
    "!eval `oidc-keychain` > /dev/null && oidc-token dodas --time=3600 > /tmp/token\n",
    "with open('/tmp/token') as f:\n",
    "    token = f.readlines()[0].split(\"\\n\")[0]\n",
    "\n",
    "# build Spark configuration options    \n",
    "conf = setupSpark(token)\n",
    "\n",
    "# to update Spark config, we need first to stop the spark context\n",
    "SparkContext.stop(sc)\n",
    "\n",
    "# Then create first spark context, and then session\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# check if Spark is there\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras training is not distributed, still we use spark to read in the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in input data\n",
    "inputFile = \"s3a://scratch/legger/higgs/Higgs100k.parquet\"\n",
    "\n",
    "%time df = spark.read.format('parquet').option('header', 'true').option('inferschema', 'true').load(inputFile)\n",
    "\n",
    "total_events = df.count()\n",
    "print('There are '+str(total_events)+' events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()\n",
    "X, y, X_test, y_test = prepareData(df, 0.2)\n",
    "\n",
    "print('Events for training '+str(len(y)))\n",
    "print('Events for validation '+str(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "- documentation: https://keras.io/getting-started/sequential-model-guide/\n",
    "- Build (compile) a Keras Sequential model (call it *model*)\n",
    "  - 1 hidden layer  with 100 neurons, activation ReLU (put in the correct input_shape!)\n",
    "  - 1 ouput layer with activation sigmoid\n",
    "  - use Adam optimiser\n",
    "  - use binary_crossentropy loss\n",
    "  - use accuracy metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's first look at NN in keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# define the model\n",
    "#model = ???\n",
    "\n",
    "#add layers\n",
    "\n",
    "# Compile model\n",
    "# model.compile(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X, y, batch_size=128, epochs=20, validation_data=(X_test, y_test), callbacks=[es]) #, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotVsEpoch(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotVsEpoch(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to get predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#draw ROC\n",
    "drawROC2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do signal vs background plot\n",
    "m_bb = X_test[:,25]\n",
    "\n",
    "def isSignal(x, y):\n",
    "    if (y>=0.5):\n",
    "        return x\n",
    "    else: \n",
    "        return -1.\n",
    "    \n",
    "def isBackground(x, y):\n",
    "    if (y<0.5):\n",
    "        return x\n",
    "    else: \n",
    "        return -1.\n",
    "isSignalNP = np.vectorize(isSignal)\n",
    "isBackgroundNP = np.vectorize(isBackground)\n",
    "\n",
    "m_bb_signal = isSignalNP(m_bb, y_test)\n",
    "m_bb_background = isBackgroundNP(m_bb, y_test)\n",
    "m_bb_signal_pred = isSignalNP(m_bb, y_pred[:,0])\n",
    "m_bb_background_pred = isBackgroundNP(m_bb, y_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "plt.hist(m_bb_signal, bins = 100, range=[0, 3.5], alpha=0.5, label='signal') \n",
    "plt.hist(m_bb_background, bins = 100, range=[0, 3.5], alpha=0.5, label='background') \n",
    "plt.hist(m_bb_signal_pred, bins = 100, range=[0, 3.5], label='predicted signal', histtype='step',\n",
    "        linestyle='--', color='green', linewidth=2) \n",
    "plt.hist(m_bb_background_pred, bins = 100, range=[0, 3.5], label='predicted background', histtype='step',\n",
    "        linestyle='--', color='red', linewidth=2) \n",
    "plt.title(\"histogram\") \n",
    "ax.set_xlabel('m_bb')\n",
    "ax.set_ylabel('counts')\n",
    "ax.legend()\n",
    "ax.set_title(\"Distribution of m_bb\")\n",
    "plt.show()\n",
    "f.savefig(\"SignalvsBackgroundPred.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "- play with the model parameters and see if you can improve the performances (try to add hidden layers)\n",
    "- try to increase the number of epochs for training, is the model improving?\n",
    "- If statistics is limited in validation sample, try to play with different ratios (60:40, 70:30)\n",
    "- Under which conditions are you overfitting?\n",
    "- how is training time affected by the number of parameters?\n",
    "- how is signal versus background separation affected (check different variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.driver.maxResultSize",
     "value": "0"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
